{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T22:55:35.447293Z",
     "start_time": "2023-06-06T22:55:35.433318500Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T22:55:35.447293Z",
     "start_time": "2023-06-06T22:55:35.439748100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rootPath = \"/\"\n",
    "relTrainCSVpath = \"CONDA/data/CONDA_train.csv\"\n",
    "relTestCSVpath = \"CONDA/data/CONDA_test.csv\"\n",
    "relValidationCSVpath = \"CONDA/data/CONDA_valid.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T22:55:35.528374500Z",
     "start_time": "2023-06-06T22:55:35.443286400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainDF = pd.read_csv(os.path.join(relTrainCSVpath))\n",
    "testDF = pd.read_csv(os.path.join(relTestCSVpath))\n",
    "valDF = pd.read_csv(os.path.join(relValidationCSVpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T22:55:35.707914600Z",
     "start_time": "2023-06-06T22:55:35.529364700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = trainDF['utterance']\n",
    "labels = trainDF['intentClass']\n",
    "d = {'texts':texts, 'labels': labels}\n",
    "train_DF = pd.DataFrame(d)\n",
    "train_DF = train_DF.dropna()\n",
    "texts = train_DF[\"texts\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T22:55:37.036768800Z",
     "start_time": "2023-06-06T22:55:35.711447200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# trainDF2 = trainDF['utterance'].map(lambda x: tokenizer(str(x), truncation=True))\n",
    "\n",
    "#trainDF['utterance'] = trainDF['utterance'].map(lambda x: tokenizer(str(x), truncation=True))\n",
    "#train_Str = trainDF.utterance.values.tolist()\n",
    "train_encodings = tokenizer(texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T22:55:37.042290700Z",
     "start_time": "2023-06-06T22:55:37.034760700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T22:55:37.227600700Z",
     "start_time": "2023-06-06T22:55:37.044729700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T22:55:37.232858900Z",
     "start_time": "2023-06-06T22:55:37.230332200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T23:01:17.045159800Z",
     "start_time": "2023-06-06T23:01:17.040771500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26916    O\n",
       "26917    O\n",
       "26918    O\n",
       "26919    O\n",
       "26920    O\n",
       "Name: labels, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(trainDF['intentClass'][:20])\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# trainDF['intentClassLabel'] = le.fit_transform(trainDF['intentClass'])\n",
    "# trainDF[:10]\n",
    "train_DF[\"labels\"].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T23:01:44.230022100Z",
     "start_time": "2023-06-06T23:01:44.221769700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2label = {0: \"A\", 1: \"E\", 2: \"I\", 3: \"O\"}\n",
    "label2id = {\"A\": 0, \"E\": 1, \"I\": 2, \"O\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T23:02:52.664624700Z",
     "start_time": "2023-06-06T23:02:32.722003600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|██████████| 268M/268M [00:23<00:00, 11.2MB/s] \n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=4, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = valDF['utterance']\n",
    "labels = valDF['intentClass']\n",
    "d = {'texts':texts, 'labels': labels}\n",
    "test_DF = pd.DataFrame(d)\n",
    "test_DF = test_DF.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        # texts = dataframe.texts.tolist()\n",
    "        # self.texts = tokenizer(texts,truncation=True, padding=True)\n",
    "        self.texts = dataframe.texts\n",
    "        self.targets = self.data.labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # texts = str(self.texts[index])\n",
    "        # texts = \" \".join(texts.split())\n",
    "\n",
    "        # # inputs = self.tokenizer.encode_plus(\n",
    "        # #     texts,\n",
    "        # #     None,\n",
    "        # #     add_special_tokens=True,\n",
    "        # #     max_length=self.max_len,\n",
    "        # #     pad_to_max_length=True,\n",
    "        # #     return_token_type_ids=True\n",
    "        # # )\n",
    "        # # ids = inputs['input_ids']\n",
    "        # mask = inputs['attention_mask']\n",
    "        # token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        # return {\n",
    "        #     'ids': torch.tensor(ids, dtype=torch.long),\n",
    "        #     'mask': torch.tensor(mask, dtype=torch.long),\n",
    "        #     'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "        #     'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        #     }\n",
    "        item = {key: torch.tensor(val[index]) for key, val in self.texts.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[index])\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3366 [23:12<?, ?it/s]\n",
      "  0%|          | 0/3366 [22:19<?, ?it/s]\n",
      "  0%|          | 0/3366 [17:19<?, ?it/s]\n",
      "  0%|          | 0/3366 [11:16<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "trainDataset = CustomDataset(train_DF,tokenizer,max_len=32)\n",
    "testDataset = CustomDataset(test_DF,tokenizer,max_len=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Karan\\Documents\\AI_COURSE\\aasd4011-2023-spring\\venv\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "new(): invalid data type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainDataset[\u001b[39m1\u001b[39;49m]\n",
      "Cell \u001b[1;32mIn[51], line 38\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     30\u001b[0m mask \u001b[39m=\u001b[39m inputs[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     31\u001b[0m token_type_ids \u001b[39m=\u001b[39m inputs[\u001b[39m\"\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     34\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[0;32m     35\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mids\u001b[39m\u001b[39m'\u001b[39m: torch\u001b[39m.\u001b[39mtensor(ids, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong),\n\u001b[0;32m     36\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m'\u001b[39m: torch\u001b[39m.\u001b[39mtensor(mask, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong),\n\u001b[0;32m     37\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m: torch\u001b[39m.\u001b[39mtensor(token_type_ids, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong),\n\u001b[1;32m---> 38\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtargets\u001b[39m\u001b[39m'\u001b[39m: torch\u001b[39m.\u001b[39;49mtensor(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtargets[index], dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat)\n\u001b[0;32m     39\u001b[0m     }\n",
      "\u001b[1;31mTypeError\u001b[0m: new(): invalid data type 'str'"
     ]
    }
   ],
   "source": [
    "trainDataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T23:04:59.584217Z",
     "start_time": "2023-06-06T23:04:55.551420600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "TypeError",
     "evalue": "new(): invalid data type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 24\u001b[0m\n\u001b[0;32m      1\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m      2\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtemp_model\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     learning_rate\u001b[39m=\u001b[39m\u001b[39m2e-5\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     push_to_hub\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     15\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     16\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics,\n\u001b[0;32m     22\u001b[0m )\n\u001b[1;32m---> 24\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\Karan\\Documents\\AI_COURSE\\aasd4011-2023-spring\\venv\\pytorch\\lib\\site-packages\\transformers\\trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1657\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1659\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1661\u001b[0m )\n\u001b[1;32m-> 1662\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1663\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1664\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1665\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1666\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1667\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Karan\\Documents\\AI_COURSE\\aasd4011-2023-spring\\venv\\pytorch\\lib\\site-packages\\transformers\\trainer.py:1899\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1896\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1898\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 1899\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1900\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1901\u001b[0m     \u001b[39mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[1;32mc:\\Users\\Karan\\Documents\\AI_COURSE\\aasd4011-2023-spring\\venv\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Karan\\Documents\\AI_COURSE\\aasd4011-2023-spring\\venv\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Karan\\Documents\\AI_COURSE\\aasd4011-2023-spring\\venv\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Karan\\Documents\\AI_COURSE\\aasd4011-2023-spring\\venv\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[51], line 38\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     30\u001b[0m mask \u001b[39m=\u001b[39m inputs[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     31\u001b[0m token_type_ids \u001b[39m=\u001b[39m inputs[\u001b[39m\"\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     34\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[0;32m     35\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mids\u001b[39m\u001b[39m'\u001b[39m: torch\u001b[39m.\u001b[39mtensor(ids, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong),\n\u001b[0;32m     36\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m'\u001b[39m: torch\u001b[39m.\u001b[39mtensor(mask, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong),\n\u001b[0;32m     37\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m: torch\u001b[39m.\u001b[39mtensor(token_type_ids, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong),\n\u001b[1;32m---> 38\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtargets\u001b[39m\u001b[39m'\u001b[39m: torch\u001b[39m.\u001b[39;49mtensor(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtargets[index], dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat)\n\u001b[0;32m     39\u001b[0m     }\n",
      "\u001b[1;31mTypeError\u001b[0m: new(): invalid data type 'str'"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"temp_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=trainDataset,\n",
    "    eval_dataset=testDataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
